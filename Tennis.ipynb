{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brain_name: TennisBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('brain_name:', brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents (num_agents): 2\n",
      "Size of each action (action_size): 2\n",
      "There are 2 agents. Each observes a state with length (state_size): 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.4669857  -1.5         0.          0.\n",
      " -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents (num_agents):', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action (action_size):', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length (state_size): {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "states[0]: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.38993645 -1.5        -0.          0.\n",
      "  6.83172083  5.99607611 -0.          0.        ]\n",
      "states[1]: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.70024681 -1.5         0.          0.\n",
      " -6.83172083  5.99607611  0.          0.        ]\n",
      "actions: [[ 0.08233786  1.        ]\n",
      " [-1.          0.01887031]]\n",
      "rewards: [0.0, 0.0]\n",
      "scores: [ 0.  0.]\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    #while True:\n",
    "    for i in range(10):\n",
    "        print(\"\\n\\nstates[0]: {}\".format(states[0]))\n",
    "        print(\"states[1]: {}\".format(states[1]))\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        print(\"actions: {}\".format(actions))\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        print(\"rewards: {}\".format(rewards))\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        print(\"scores: {}\".format(scores))\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "        break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define netowrks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import random\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "# actor - take in a state and output a distribution of actions\n",
    "class ActorModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.out = torch.nn.Linear(256, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, states):\n",
    "        batch_size = states.size(0)\n",
    "        x = self.fc1(states)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.tanh(self.out(x))\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "# critic - take in a state AND actions - outputs a state value function - V\n",
    "class CriticModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc2 = torch.nn.Linear(256+action_size, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = torch.nn.Linear(256, 128)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        self.out = torch.nn.Linear(128, 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        batch_size = states.size(0)\n",
    "        xs = F.leaky_relu(self.bn1(self.fc1(states)))\n",
    "        x = torch.cat((xs, actions), dim=1) #add in actions to the network\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "\n",
    "class StepInfo:\n",
    "    def __init__(self, step_number, states, actions, rewards, next_states, dones):\n",
    "        self.step_number = step_number\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        self.dones = dones\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"step_number: {},  states: {},  actions: {},  rewards: {},  next_states: {}\".format(self.step_number, self.states, self.actions, self.rewards, self.next_states)\n",
    "\n",
    "def reset_game(in_env, brain_name):\n",
    "    # **important note** When training the environment, set `train_mode=True`\n",
    "    env_info = in_env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations\n",
    "    return states\n",
    "\n",
    "def env_step(in_env, brain_name, states, replay_buffer, actor_model_0, actor_model_1, epsilon, logging=False):\n",
    "    #Play a game. Add to the replay_buffer\n",
    "    if (len(replay_buffer) > 0):\n",
    "        step_number = replay_buffer[-1].step_number + 1\n",
    "    else:\n",
    "        step_number = 0\n",
    "\n",
    "    state_tensor = torch.from_numpy(states).float().cuda()\n",
    "\n",
    "    rand_num = random.uniform(0, 1)\n",
    "    if rand_num > epsilon:\n",
    "        actor_model_0.eval()\n",
    "        actor_model_1.eval()\n",
    "        #with torch.no_grad():\n",
    "        #print(\"state_tensor[0].unsqueeze(0): {}\".format(state_tensor[0].unsqueeze(0)))\n",
    "        actions_tensor_0 = actor_model_0(state_tensor[0].unsqueeze(0))\n",
    "        actions_tensor_1 = actor_model_1(state_tensor[1].unsqueeze(0))\n",
    "        actor_model_0.train()\n",
    "        actor_model_1.train()\n",
    "        actions_np = np.zeros((num_agents, action_size))\n",
    "        actions_np[0] = actions_tensor_0.detach().cpu().numpy()\n",
    "        actions_np[1] = actions_tensor_1.detach().cpu().numpy()\n",
    "        if logging:\n",
    "            print(\"actions from models: {}\".format(actions_np))\n",
    "    else:\n",
    "        actions_np =  (2.0 * np.random.rand(num_agents, action_size)) - 1.0\n",
    "        if logging:\n",
    "            print(\"random actions: {}\".format(actions_np))\n",
    "\n",
    "    env_info = in_env.step(actions_np)[brain_name]      # send all actions to the environment\n",
    "    next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "    rewards = env_info.rewards                          # get reward (for each agent)\n",
    "    dones = env_info.local_done                         # see if episode finished\n",
    "\n",
    "    this_step_info = StepInfo(step_number, states, actions_np, rewards, next_states, dones)\n",
    "    replay_buffer.append(this_step_info)\n",
    "\n",
    "    return next_states\n",
    "\n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "def getBatch(replay_buffer, batch_size):\n",
    "#     Number of agents (num_agents): 2\n",
    "#     Size of each action (action_size): 2\n",
    "#     There are 2 agents. Each observes a state with length (state_size): 24\n",
    "    return_states = np.zeros((batch_size, num_agents, state_size))\n",
    "    return_actions = np.zeros((batch_size, num_agents, action_size))\n",
    "    return_rewards = np.zeros((batch_size, 2))\n",
    "    return_next_states = np.zeros((batch_size, num_agents, state_size))\n",
    "    return_next_actions = np.zeros((batch_size, num_agents, action_size))\n",
    "    \n",
    "#     print(\"replay_buffer[0].states.shape: {}\".format(replay_buffer[0].states.shape))\n",
    "#     print(\"replay_buffer[0].rewards[0]: {}\".format(replay_buffer[0].rewards[0]))\n",
    "#     print(\"replay_buffer[0].actions.shape: {}\".format(replay_buffer[0].actions.shape))\n",
    "#     print(\"replay_buffer[0].next_states.shape: {}\".format(replay_buffer[0].next_states.shape))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rand_frame_index = random.randint(0,len(replay_buffer)-2)\n",
    "        return_states[i] = replay_buffer[rand_frame_index].states[0]\n",
    "        return_actions[i] = replay_buffer[rand_frame_index].actions[0]\n",
    "        return_rewards[i] = replay_buffer[rand_frame_index].rewards[0]\n",
    "        return_next_states[i] = replay_buffer[rand_frame_index].next_states[0]\n",
    "        return_next_actions[i] = replay_buffer[rand_frame_index+1].actions[0]\n",
    "        #### TODO - make sure \"next\" actions don't roll over onto the next  playthrough.\n",
    "        \n",
    "    return return_states, return_actions, return_rewards, return_next_states, return_next_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from disk\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "# instantiate objects that will can be re-used\n",
    "buffer_length = 100000\n",
    "replay_buffer = deque(maxlen=buffer_length)\n",
    "\n",
    "actor_model_local_0   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target_0  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local_0  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target_0 = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "actor_model_local_1   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target_1  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local_1  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target_1 = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "actor_model_locals   = (actor_model_local_0, actor_model_local_1)\n",
    "actor_model_targets  = (actor_model_target_0, actor_model_target_1)\n",
    "critic_model_locals  = (critic_model_local_0, critic_model_local_1)\n",
    "critic_model_targets = (critic_model_target_0, critic_model_target_1)\n",
    "\n",
    "if True:  # set to false if running for the first time or fresh models are desired.\n",
    "    print(\"Loading models from disk\")\n",
    "    actor_model_locals[0].load_state_dict(torch.load(\"actor_model_local_0.pt\"))\n",
    "    actor_model_targets[0].load_state_dict(torch.load(\"actor_model_target_0.pt\"))\n",
    "    critic_model_locals[0].load_state_dict(torch.load(\"critic_model_local_0.pt\"))\n",
    "    critic_model_targets[0].load_state_dict(torch.load(\"critic_model_target_0.pt\"))\n",
    "    \n",
    "    actor_model_locals[1].load_state_dict(torch.load(\"actor_model_local_1.pt\"))\n",
    "    actor_model_targets[1].load_state_dict(torch.load(\"actor_model_target_1.pt\"))\n",
    "    critic_model_locals[1].load_state_dict(torch.load(\"critic_model_local_1.pt\"))\n",
    "    critic_model_targets[1].load_state_dict(torch.load(\"critic_model_target_1.pt\"))\n",
    "\n",
    "lr_actor = .0005 #learning rate of .001 is too high - losses going in wrong direction.\n",
    "lr_critic = .0005\n",
    "weight_decay = 0.0\n",
    "actor_optimizer_0 = optim.Adam(actor_model_locals[0].parameters(), lr=lr_actor)\n",
    "actor_optimizer_1 = optim.Adam(actor_model_locals[1].parameters(), lr=lr_actor)\n",
    "critic_optimizer_0 = optim.Adam(critic_model_locals[0].parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "critic_optimizer_1 = optim.Adam(critic_model_locals[1].parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "\n",
    "actor_optimizers = (actor_optimizer_0, actor_optimizer_1)\n",
    "critic_optimizers = (critic_optimizer_0, critic_optimizer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_player(player_idx, batch_size, gamma, tau):\n",
    "    #do some learning\n",
    "    states, actions, rewards, next_states, next_actions = getBatch(replay_buffer, batch_size)\n",
    "\n",
    "#     print(\"batch states.shape: {}\".format(states.shape)) #(128, 2, 24)\n",
    "#     print(\"batch actions.shape: {}\".format(actions.shape)) #(128, 2, 2)\n",
    "#     print(\"batch rewards.shape: {}\".format(rewards.shape)) #(128, 2)\n",
    "#     print(\"batch next_states.shape: {}\".format(next_states.shape)) #(128, 2, 24)\n",
    "#     print(\"batch next_actions.shape: {}\".format(next_actions.shape)) #(128, 2, 2)\n",
    "\n",
    "    # convert to tensors for input into the models.\n",
    "    rewards_tensor = torch.from_numpy(rewards[:,player_idx]).unsqueeze(1).float().cuda()\n",
    "#     print(\"rewards_tensor.shape: {}\".format(rewards_tensor.shape))\n",
    "    \n",
    "    next_actions_tensor = torch.from_numpy(next_actions[:,player_idx,:]).float().cuda()\n",
    "#     print(\"next_actions_tensor.shape: {}\".format(next_actions_tensor.shape))\n",
    "    \n",
    "    next_states_tensor = torch.from_numpy(next_states[:,player_idx,:]).float().cuda()\n",
    "#     print(\"next_states_tensor.shape: {}\".format(next_states_tensor.shape))\n",
    "    \n",
    "    states_tensor = torch.from_numpy(states[:,player_idx,:]).float().cuda()\n",
    "#     print(\"states_tensor.shape: {}\".format(states_tensor.shape))\n",
    "    \n",
    "    actions_tensor = torch.from_numpy(actions[:,player_idx,:]).float().cuda()\n",
    "#     print(\"actions_tensor.shape: {}\".format(actions_tensor.shape))\n",
    "\n",
    "    # ---------------------------- update critic ---------------------------- #\n",
    "    # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "    # Compute Q targets for current states (y_i)\n",
    "    actor_model_targets[player_idx].eval()\n",
    "    actor_model_locals[player_idx].eval()\n",
    "    critic_model_targets[player_idx].eval()\n",
    "    critic_model_locals[player_idx].eval()\n",
    "    \n",
    "    actions_next = actor_model_targets[player_idx](next_states_tensor)\n",
    "    Q_targets_next = critic_model_targets[player_idx](next_states_tensor, actions_next)\n",
    "    Q_targets = rewards_tensor + (gamma * Q_targets_next)\n",
    "\n",
    "    # Compute critic loss\n",
    "    Q_expected = critic_model_locals[player_idx](states_tensor, actions_tensor)\n",
    "    critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "    # Minimize the critic loss\n",
    "    critic_model_locals[player_idx].train()\n",
    "    critic_optimizers[player_idx].zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic_model_locals[player_idx].parameters(), 1)\n",
    "    critic_optimizers[player_idx].step()\n",
    "\n",
    "    # ---------------------------- update actor ---------------------------- #\n",
    "    # Compute actor loss\n",
    "    actions_pred = actor_model_locals[player_idx](states_tensor)\n",
    "    critic_model_locals[player_idx].eval()\n",
    "    actor_loss = -critic_model_locals[player_idx](states_tensor, actions_pred).mean()\n",
    "    \n",
    "    critic_model_locals[player_idx].train()\n",
    "    critic_model_targets[player_idx].train()\n",
    "    actor_model_targets[player_idx].train()\n",
    "    actor_model_locals[player_idx].train()\n",
    "    \n",
    "    # Minimize the actor loss\n",
    "    actor_optimizers[player_idx].zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizers[player_idx].step()\n",
    "\n",
    "    # ----------------------- update target networks ----------------------- #\n",
    "    #use very small Tau and update with every step\n",
    "    soft_update_target(critic_model_locals[player_idx], critic_model_targets[player_idx], tau)\n",
    "    soft_update_target(actor_model_locals[player_idx], actor_model_targets[player_idx], tau)\n",
    "\n",
    "    return critic_loss.item(), actor_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - epsilon: 0.100\n",
      "    total_rewards_0: 2.160, critic_loss_0: 0.00002368, actor_loss_0: -0.340311\n",
      "    total_rewards_1: 2.050, critic_loss_1: 0.00002250, actor_loss_1: -0.336133\n",
      "total_play_rewards_0: 2.670, total_play_rewards_1: 2.260\n",
      "epoch: 1 - epsilon: 0.100\n",
      "    total_rewards_0: 2.190, critic_loss_0: 0.00002407, actor_loss_0: -0.339907\n",
      "    total_rewards_1: 2.480, critic_loss_1: 0.00003630, actor_loss_1: -0.334128\n",
      "total_play_rewards_0: 2.380, total_play_rewards_1: 2.110\n",
      "epoch: 2 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002123, actor_loss_0: -0.339315\n",
      "    total_rewards_1: 1.970, critic_loss_1: 0.00004168, actor_loss_1: -0.331701\n",
      "total_play_rewards_0: 2.270, total_play_rewards_1: 2.090\n",
      "epoch: 3 - epsilon: 0.100\n",
      "    total_rewards_0: 3.400, critic_loss_0: 0.00002291, actor_loss_0: -0.338896\n",
      "    total_rewards_1: 1.690, critic_loss_1: 0.00002849, actor_loss_1: -0.331835\n",
      "total_play_rewards_0: 2.140, total_play_rewards_1: 2.060\n",
      "epoch: 4 - epsilon: 0.100\n",
      "    total_rewards_0: 2.470, critic_loss_0: 0.00002421, actor_loss_0: -0.338372\n",
      "    total_rewards_1: 2.060, critic_loss_1: 0.00002465, actor_loss_1: -0.331308\n",
      "total_play_rewards_0: 2.570, total_play_rewards_1: 1.990\n",
      "epoch: 5 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002309, actor_loss_0: -0.337989\n",
      "    total_rewards_1: 2.090, critic_loss_1: 0.00002524, actor_loss_1: -0.330871\n",
      "total_play_rewards_0: 2.280, total_play_rewards_1: 2.350\n",
      "epoch: 6 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00002191, actor_loss_0: -0.337389\n",
      "    total_rewards_1: 2.470, critic_loss_1: 0.00002391, actor_loss_1: -0.330247\n",
      "total_play_rewards_0: 2.750, total_play_rewards_1: 2.350\n",
      "epoch: 7 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002208, actor_loss_0: -0.337034\n",
      "    total_rewards_1: 2.100, critic_loss_1: 0.00002285, actor_loss_1: -0.329933\n",
      "total_play_rewards_0: 2.600, total_play_rewards_1: 2.170\n",
      "epoch: 8 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002225, actor_loss_0: -0.336413\n",
      "    total_rewards_1: 2.450, critic_loss_1: 0.00002307, actor_loss_1: -0.329617\n",
      "total_play_rewards_0: 2.680, total_play_rewards_1: 2.180\n",
      "epoch: 9 - epsilon: 0.100\n",
      "    total_rewards_0: 2.890, critic_loss_0: 0.00002199, actor_loss_0: -0.335985\n",
      "    total_rewards_1: 2.190, critic_loss_1: 0.00002242, actor_loss_1: -0.329238\n",
      "total_play_rewards_0: 2.490, total_play_rewards_1: 2.300\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "epochs = 10 # 5\n",
    "steps_per_epoch = 1000\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "batch_size = 128\n",
    "epsilon_decay_steps = buffer_length\n",
    "epsilon_max = 0.75\n",
    "epsilon_min = 0.1\n",
    "current_state = reset_game(env, brain_name)\n",
    "\n",
    "with active_session():\n",
    "    for epoch in range(epochs):\n",
    "        total_actor_loss_0 = 0.0\n",
    "        total_critic_loss_0 = 0.0    \n",
    "        total_actor_loss_1 = 0.0\n",
    "        total_critic_loss_1 = 0.0    \n",
    "        total_rewards_0 = 0\n",
    "        total_rewards_1 = 0\n",
    "        epsilon = max(epsilon_min, epsilon_max - len(replay_buffer)/epsilon_decay_steps)\n",
    "\n",
    "        for game_step in range(steps_per_epoch):\n",
    "            #play a game\n",
    "            #total_rewards = play_game(env, brain_name, replay_buffer, actor_model_local)\n",
    "            current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "            total_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "            total_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "\n",
    "            #if the game is done, reset and continue\n",
    "            if np.any(replay_buffer[-1].dones):\n",
    "                # new game\n",
    "                current_state = reset_game(env, brain_name)\n",
    "                current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "                total_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "                total_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "\n",
    "            if len(replay_buffer) < 10000:\n",
    "                continue  \n",
    "\n",
    "            critic_loss_0, actor_loss_0  = train_player(0, batch_size, gamma, tau)\n",
    "            critic_loss_1, actor_loss_1  = train_player(1, batch_size, gamma, tau)\n",
    "\n",
    "            total_actor_loss_0 += actor_loss_0\n",
    "            total_actor_loss_1 += actor_loss_1\n",
    "            total_critic_loss_0 += critic_loss_0\n",
    "            total_critic_loss_1 += critic_loss_1\n",
    "\n",
    "        print(\"epoch: {} - epsilon: {:.3f}\".format(epoch, epsilon))\n",
    "        print(\"    total_rewards_0: {:.3f}, critic_loss_0: {:.8f}, actor_loss_0: {:.6f}\".format(total_rewards_0, total_critic_loss_0/steps_per_epoch, total_actor_loss_0/steps_per_epoch))\n",
    "        print(\"    total_rewards_1: {:.3f}, critic_loss_1: {:.8f}, actor_loss_1: {:.6f}\".format(total_rewards_1, total_critic_loss_1/steps_per_epoch, total_actor_loss_1/steps_per_epoch))\n",
    "        \n",
    "        total_play_rewards_0 = 0\n",
    "        total_play_rewards_1 = 0\n",
    "        for game_step in range(steps_per_epoch):\n",
    "            #play a game\n",
    "            current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "            total_play_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "            total_play_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "\n",
    "            #if the game is done, reset and continue\n",
    "            if np.any(replay_buffer[-1].dones):\n",
    "                # new game\n",
    "                current_state = reset_game(env, brain_name)\n",
    "                current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "                total_play_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "                total_play_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "        \n",
    "        print(\"    total_play_rewards_0: {:.3f}, total_play_rewards_1: {:.3f}\".format(total_play_rewards_0, total_play_rewards_1))\n",
    "        if total_play_rewards_0 > total_play_rewards_1:\n",
    "            soft_update_target(actor_model_locals[0], actor_model_locals[1], 1.0)\n",
    "            soft_update_target(actor_model_targets[0], actor_model_locals[1], 1.0)\n",
    "            soft_update_target(critic_model_locals[0], actor_model_locals[1], 1.0)\n",
    "            soft_update_target(critic_model_targets[0], actor_model_locals[1], 1.0)\n",
    "        else:\n",
    "            soft_update_target(actor_model_locals[1], actor_model_locals[0], 1.0)\n",
    "            soft_update_target(actor_model_targets[1], actor_model_locals[0], 1.0)\n",
    "            soft_update_target(critic_model_locals[1], actor_model_locals[0], 1.0)\n",
    "            soft_update_target(critic_model_targets[1], actor_model_locals[0], 1.0)\n",
    "            \n",
    "\n",
    "    torch.save(actor_model_locals[0].state_dict(),  \"actor_model_local_0.pt\")\n",
    "    torch.save(actor_model_targets[0].state_dict(), \"actor_model_target_0.pt\")\n",
    "    torch.save(critic_model_locals[0].state_dict(), \"critic_model_local_0.pt\")\n",
    "    torch.save(critic_model_targets[0].state_dict(),\"critic_model_target_0.pt\")\n",
    "\n",
    "    torch.save(actor_model_locals[1].state_dict(),  \"actor_model_local_1.pt\")\n",
    "    torch.save(actor_model_targets[1].state_dict(), \"actor_model_target_1.pt\")\n",
    "    torch.save(critic_model_locals[1].state_dict(), \"critic_model_local_1.pt\")\n",
    "    torch.save(critic_model_targets[1].state_dict(),\"critic_model_target_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1, total_rewards_agent_0: 0.800, total_rewards_agent_1: 0.790, game steps: 314\n",
      "game 2, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 3, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 4, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 37\n",
      "game 5, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 6, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 7, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 31\n",
      "game 8, total_rewards_agent_0: 0.400, total_rewards_agent_1: 0.390, game steps: 155\n",
      "game 9, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 37\n",
      "game 10, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 11, total_rewards_agent_0: 1.990, total_rewards_agent_1: 1.900, game steps: 751\n",
      "game 12, total_rewards_agent_0: 0.200, total_rewards_agent_1: 0.190, game steps: 85\n",
      "game 13, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 14, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 15, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 16, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 17, total_rewards_agent_0: 0.200, total_rewards_agent_1: 0.090, game steps: 76\n",
      "game 18, total_rewards_agent_0: -0.010, total_rewards_agent_1: 0.000, game steps: 13\n",
      "game 19, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 20, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 21, total_rewards_agent_0: 0.200, total_rewards_agent_1: 0.190, game steps: 68\n",
      "game 22, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 23\n",
      "game 23, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 24, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 25, total_rewards_agent_0: 1.400, total_rewards_agent_1: 1.390, game steps: 534\n",
      "game 26, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 27, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 28, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 29, total_rewards_agent_0: 1.300, total_rewards_agent_1: 1.190, game steps: 482\n",
      "game 30, total_rewards_agent_0: 0.000, total_rewards_agent_1: -0.010, game steps: 19\n",
      "game 31, total_rewards_agent_0: 0.290, total_rewards_agent_1: 0.300, game steps: 127\n",
      "game 32, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 32\n",
      "game 33, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 34, total_rewards_agent_0: 2.100, total_rewards_agent_1: 2.090, game steps: 811\n",
      "game 35, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 36, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 32\n",
      "game 37, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 38, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 22\n",
      "game 39, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 40, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 41, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 42, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 43, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 36\n",
      "game 44, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 45, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 46, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 47, total_rewards_agent_0: 0.390, total_rewards_agent_1: 0.500, game steps: 183\n",
      "game 48, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 37\n",
      "game 49, total_rewards_agent_0: 0.000, total_rewards_agent_1: 0.090, game steps: 31\n",
      "game 50, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 22\n",
      "game 51, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 52, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 53, total_rewards_agent_0: 0.000, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 54, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 41\n",
      "game 55, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 56, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 57, total_rewards_agent_0: -0.010, total_rewards_agent_1: 0.000, game steps: 14\n",
      "game 58, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 59, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 30\n",
      "game 60, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 61, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 62, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 37\n",
      "game 63, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 31\n",
      "game 64, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 65, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 66, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 67, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 36\n",
      "game 68, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 32\n",
      "game 69, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 70, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 71, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.700, game steps: 1001\n",
      "game 72, total_rewards_agent_0: 2.600, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 73, total_rewards_agent_0: 2.700, total_rewards_agent_1: 2.600, game steps: 1001\n",
      "game 74, total_rewards_agent_0: 0.090, total_rewards_agent_1: 0.100, game steps: 45\n",
      "game 75, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 76, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 77, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 18\n",
      "game 78, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 79, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 20\n",
      "game 80, total_rewards_agent_0: 1.990, total_rewards_agent_1: 2.000, game steps: 775\n",
      "game 81, total_rewards_agent_0: 0.800, total_rewards_agent_1: 0.790, game steps: 318\n",
      "game 82, total_rewards_agent_0: -0.010, total_rewards_agent_1: 0.100, game steps: 34\n",
      "game 83, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 84, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 85, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 86, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 33\n",
      "game 87, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 88, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 89, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 36\n",
      "game 90, total_rewards_agent_0: 0.000, total_rewards_agent_1: 0.090, game steps: 32\n",
      "game 91, total_rewards_agent_0: -0.010, total_rewards_agent_1: 0.100, game steps: 31\n",
      "game 92, total_rewards_agent_0: 0.000, total_rewards_agent_1: -0.010, game steps: 13\n",
      "game 93, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 37\n",
      "game 94, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 95, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 35\n",
      "game 96, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "game 97, total_rewards_agent_0: 0.100, total_rewards_agent_1: -0.010, game steps: 31\n",
      "game 98, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 35\n",
      "game 99, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 100, total_rewards_agent_0: 0.100, total_rewards_agent_1: 0.090, game steps: 34\n",
      "Average max reward over 100 games: 0.740\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.05\n",
    "total_game_count = 100\n",
    "total_rewards_over_all_games_agent_0 = 0\n",
    "total_rewards_over_all_games_agent_1 = 0\n",
    "total_max_over_all_games = 0\n",
    "\n",
    "for game_count in range(total_game_count):\n",
    "    total_rewards_agent_0 = 0.0\n",
    "    total_rewards_agent_1 = 0.0\n",
    "\n",
    "    current_state = reset_game(env, brain_name)\n",
    "    replay = []\n",
    "    \n",
    "    for i in range(2000):\n",
    "        current_state = env_step(env, brain_name, current_state, replay, actor_model_locals[0], actor_model_locals[1], epsilon, logging=False)\n",
    "        total_rewards_agent_0 += replay[-1].rewards[0]\n",
    "        total_rewards_agent_1 += replay[-1].rewards[1]\n",
    "        total_max_over_all_games += max(replay[-1].rewards[0], replay[-1].rewards[1])\n",
    "\n",
    "        total_rewards_over_all_games_agent_0 += total_rewards_agent_0\n",
    "        total_rewards_over_all_games_agent_1 += total_rewards_agent_1\n",
    "        \n",
    "        if np.any(replay[-1].dones):\n",
    "            break\n",
    "        \n",
    "    print(\"game {}, total_rewards_agent_0: {:.3f}, total_rewards_agent_1: {:.3f}, game steps: {}\".format(game_count+1, total_rewards_agent_0, total_rewards_agent_1, len(replay)))\n",
    "    \n",
    "print(\"Average max reward over {} games: {:.3f}\".format(total_game_count, total_max_over_all_games/total_game_count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    stepInfo = replay_buffer[i]\n",
    "    print(\"\\nstep_number: {}\".format(stepInfo.step_number))\n",
    "    print(\"states: {}\".format(stepInfo.states))\n",
    "    print(\"actions: {}\".format(stepInfo.actions))\n",
    "    print(\"rewards: {}\".format(stepInfo.rewards))\n",
    "    print(\"next_states: {}\".format(stepInfo.next_states))\n",
    "    print(\"dones: {}\".format(stepInfo.dones))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
