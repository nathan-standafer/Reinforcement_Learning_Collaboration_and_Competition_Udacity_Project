{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.5 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brain_name: TennisBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('brain_name:', brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents (num_agents): 2\n",
      "Size of each action (action_size): 2\n",
      "There are 2 agents. Each observes a state with length (state_size): 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.4669857  -1.5         0.          0.\n",
      " -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents (num_agents):', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action (action_size):', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length (state_size): {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "states[0]: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -7.38993645 -1.5        -0.          0.\n",
      "  6.83172083  5.99607611 -0.          0.        ]\n",
      "states[1]: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.70024681 -1.5         0.          0.\n",
      " -6.83172083  5.99607611  0.          0.        ]\n",
      "actions: [[ 0.95697224  1.        ]\n",
      " [-0.77069054 -0.13612307]]\n",
      "rewards: [0.0, 0.0]\n",
      "scores: [ 0.  0.]\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    #while True:\n",
    "    for i in range(10):\n",
    "        print(\"\\n\\nstates[0]: {}\".format(states[0]))\n",
    "        print(\"states[1]: {}\".format(states[1]))\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        print(\"actions: {}\".format(actions))\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        print(\"rewards: {}\".format(rewards))\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        print(\"scores: {}\".format(scores))\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "        break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define netowrks\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import random\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "# actor - take in a state and output a distribution of actions\n",
    "class ActorModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc2 = torch.nn.Linear(256, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.out = torch.nn.Linear(256, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, states):\n",
    "        batch_size = states.size(0)\n",
    "        x = self.fc1(states)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = F.tanh(self.out(x))\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "# critic - take in a state AND actions - outputs a state value function - V\n",
    "class CriticModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(CriticModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 256)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc2 = torch.nn.Linear(256+action_size, 256)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(256)\n",
    "        self.fc3 = torch.nn.Linear(256, 128)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        self.out = torch.nn.Linear(128, 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        batch_size = states.size(0)\n",
    "        xs = F.leaky_relu(self.bn1(self.fc1(states)))\n",
    "        x = torch.cat((xs, actions), dim=1) #add in actions to the network\n",
    "        x = F.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = F.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.out.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "\n",
    "class StepInfo:\n",
    "    def __init__(self, step_number, states, actions, rewards, next_states, dones):\n",
    "        self.step_number = step_number\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.next_states = next_states\n",
    "        self.dones = dones\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"step_number: {},  states: {},  actions: {},  rewards: {},  next_states: {}\".format(self.step_number, self.states, self.actions, self.rewards, self.next_states)\n",
    "\n",
    "def reset_game(in_env, brain_name):\n",
    "    # **important note** When training the environment, set `train_mode=True`\n",
    "    env_info = in_env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "    states = env_info.vector_observations\n",
    "    return states\n",
    "\n",
    "def env_step(in_env, brain_name, states, replay_buffer, actor_model_0, actor_model_1, epsilon, logging=False):\n",
    "    #Play a game. Add to the replay_buffer\n",
    "    if (len(replay_buffer) > 0):\n",
    "        step_number = replay_buffer[-1].step_number + 1\n",
    "    else:\n",
    "        step_number = 0\n",
    "\n",
    "    state_tensor = torch.from_numpy(states).float().cuda()\n",
    "\n",
    "    rand_num = random.uniform(0, 1)\n",
    "    if rand_num > epsilon:\n",
    "        actor_model_0.eval()\n",
    "        actor_model_1.eval()\n",
    "        #with torch.no_grad():\n",
    "        #print(\"state_tensor[0].unsqueeze(0): {}\".format(state_tensor[0].unsqueeze(0)))\n",
    "        actions_tensor_0 = actor_model_0(state_tensor[0].unsqueeze(0))\n",
    "        actions_tensor_1 = actor_model_1(state_tensor[1].unsqueeze(0))\n",
    "        actor_model_0.train()\n",
    "        actor_model_1.train()\n",
    "        actions_np = np.zeros((num_agents, action_size))\n",
    "        actions_np[0] = actions_tensor_0.detach().cpu().numpy()\n",
    "        actions_np[1] = actions_tensor_1.detach().cpu().numpy()\n",
    "        if logging:\n",
    "            print(\"actions from models: {}\".format(actions_np))\n",
    "    else:\n",
    "        actions_np =  (2.0 * np.random.rand(num_agents, action_size)) - 1.0\n",
    "        if logging:\n",
    "            print(\"random actions: {}\".format(actions_np))\n",
    "\n",
    "    env_info = in_env.step(actions_np)[brain_name]      # send all actions to the environment\n",
    "    next_states = env_info.vector_observations          # get next state (for each agent)\n",
    "    rewards = env_info.rewards                          # get reward (for each agent)\n",
    "    dones = env_info.local_done                         # see if episode finished\n",
    "\n",
    "    this_step_info = StepInfo(step_number, states, actions_np, rewards, next_states, dones)\n",
    "    replay_buffer.append(this_step_info)\n",
    "\n",
    "    return next_states\n",
    "\n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "def getBatch(replay_buffer, batch_size):\n",
    "#     Number of agents (num_agents): 2\n",
    "#     Size of each action (action_size): 2\n",
    "#     There are 2 agents. Each observes a state with length (state_size): 24\n",
    "    return_states = np.zeros((batch_size, num_agents, state_size))\n",
    "    return_actions = np.zeros((batch_size, num_agents, action_size))\n",
    "    return_rewards = np.zeros((batch_size, 2))\n",
    "    return_next_states = np.zeros((batch_size, num_agents, state_size))\n",
    "    return_next_actions = np.zeros((batch_size, num_agents, action_size))\n",
    "    \n",
    "#     print(\"replay_buffer[0].states.shape: {}\".format(replay_buffer[0].states.shape))\n",
    "#     print(\"replay_buffer[0].rewards[0]: {}\".format(replay_buffer[0].rewards[0]))\n",
    "#     print(\"replay_buffer[0].actions.shape: {}\".format(replay_buffer[0].actions.shape))\n",
    "#     print(\"replay_buffer[0].next_states.shape: {}\".format(replay_buffer[0].next_states.shape))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        rand_frame_index = random.randint(0,len(replay_buffer)-2)\n",
    "        return_states[i] = replay_buffer[rand_frame_index].states[0]\n",
    "        return_actions[i] = replay_buffer[rand_frame_index].actions[0]\n",
    "        return_rewards[i] = replay_buffer[rand_frame_index].rewards[0]\n",
    "        return_next_states[i] = replay_buffer[rand_frame_index].next_states[0]\n",
    "        return_next_actions[i] = replay_buffer[rand_frame_index+1].actions[0]\n",
    "        #### TODO - make sure \"next\" actions don't roll over onto the next  playthrough.\n",
    "        \n",
    "    return return_states, return_actions, return_rewards, return_next_states, return_next_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "# instantiate objects that will can be re-used\n",
    "buffer_length = 100000\n",
    "replay_buffer = deque(maxlen=buffer_length)\n",
    "\n",
    "actor_model_local_0   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target_0  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local_0  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target_0 = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "actor_model_local_1   = ActorModel(state_size, action_size).cuda()\n",
    "actor_model_target_1  = ActorModel(state_size, action_size).cuda()\n",
    "critic_model_local_1  = CriticModel(state_size, action_size).cuda()\n",
    "critic_model_target_1 = CriticModel(state_size, action_size).cuda()\n",
    "\n",
    "actor_model_locals   = (actor_model_local_0, actor_model_local_1)\n",
    "actor_model_targets  = (actor_model_target_0, actor_model_target_1)\n",
    "critic_model_locals  = (critic_model_local_0, critic_model_local_1)\n",
    "critic_model_targets = (critic_model_target_0, critic_model_target_1)\n",
    "\n",
    "if False:  # set to false if running for the first time or fresh models are desired.\n",
    "    actor_model_locals[0].load_state_dict(torch.load(\"actor_model_local_0.pt\"))\n",
    "    actor_model_targets[0].load_state_dict(torch.load(\"actor_model_target_0.pt\"))\n",
    "    critic_model_locals[0].load_state_dict(torch.load(\"critic_model_local_0.pt\"))\n",
    "    critic_model_targets[0].load_state_dict(torch.load(\"critic_model_target_0.pt\"))\n",
    "    \n",
    "    actor_model_locals[1].load_state_dict(torch.load(\"actor_model_local_1.pt\"))\n",
    "    actor_model_targets[1].load_state_dict(torch.load(\"actor_model_target_1.pt\"))\n",
    "    critic_model_locals[1].load_state_dict(torch.load(\"critic_model_local_1.pt\"))\n",
    "    critic_model_targets[1].load_state_dict(torch.load(\"critic_model_target_1.pt\"))\n",
    "\n",
    "    \n",
    "lr_actor = .0001\n",
    "lr_critic = .0002\n",
    "weight_decay = 0.0\n",
    "actor_optimizer_0 = optim.Adam(actor_model_locals[0].parameters(), lr=lr_actor)\n",
    "actor_optimizer_1 = optim.Adam(actor_model_locals[1].parameters(), lr=lr_actor)\n",
    "critic_optimizer_0 = optim.Adam(critic_model_locals[0].parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "critic_optimizer_1 = optim.Adam(critic_model_locals[1].parameters(), lr=lr_critic, weight_decay=weight_decay)\n",
    "\n",
    "actor_optimizers = (actor_optimizer_0, actor_optimizer_1)\n",
    "critic_optimizers = (critic_optimizer_0, critic_optimizer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_player(player_idx, batch_size, gamma, tau):\n",
    "    #do some learning\n",
    "    states, actions, rewards, next_states, next_actions = getBatch(replay_buffer, batch_size)\n",
    "\n",
    "#     print(\"batch states.shape: {}\".format(states.shape)) #(128, 2, 24)\n",
    "#     print(\"batch actions.shape: {}\".format(actions.shape)) #(128, 2, 2)\n",
    "#     print(\"batch rewards.shape: {}\".format(rewards.shape)) #(128, 2)\n",
    "#     print(\"batch next_states.shape: {}\".format(next_states.shape)) #(128, 2, 24)\n",
    "#     print(\"batch next_actions.shape: {}\".format(next_actions.shape)) #(128, 2, 2)\n",
    "\n",
    "    # convert to tensors for input into the models.\n",
    "    rewards_tensor = torch.from_numpy(rewards[:,player_idx]).unsqueeze(1).float().cuda()\n",
    "#     print(\"rewards_tensor.shape: {}\".format(rewards_tensor.shape))\n",
    "    \n",
    "    next_actions_tensor = torch.from_numpy(next_actions[:,player_idx,:]).float().cuda()\n",
    "#     print(\"next_actions_tensor.shape: {}\".format(next_actions_tensor.shape))\n",
    "    \n",
    "    next_states_tensor = torch.from_numpy(next_states[:,player_idx,:]).float().cuda()\n",
    "#     print(\"next_states_tensor.shape: {}\".format(next_states_tensor.shape))\n",
    "    \n",
    "    states_tensor = torch.from_numpy(states[:,player_idx,:]).float().cuda()\n",
    "#     print(\"states_tensor.shape: {}\".format(states_tensor.shape))\n",
    "    \n",
    "    actions_tensor = torch.from_numpy(actions[:,player_idx,:]).float().cuda()\n",
    "#     print(\"actions_tensor.shape: {}\".format(actions_tensor.shape))\n",
    "\n",
    "    # ---------------------------- update critic ---------------------------- #\n",
    "    # Get predicted next-state actions and Q values from target models\n",
    "\n",
    "    # Compute Q targets for current states (y_i)\n",
    "    actor_model_targets[player_idx].eval()\n",
    "    actor_model_locals[player_idx].eval()\n",
    "    critic_model_targets[player_idx].eval()\n",
    "    critic_model_locals[player_idx].eval()\n",
    "    \n",
    "    actions_next = actor_model_targets[player_idx](next_states_tensor)\n",
    "    Q_targets_next = critic_model_targets[player_idx](next_states_tensor, actions_next)\n",
    "    Q_targets = rewards_tensor + (gamma * Q_targets_next)\n",
    "\n",
    "    # Compute critic loss\n",
    "    Q_expected = critic_model_locals[player_idx](states_tensor, actions_tensor)\n",
    "    critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "    # Minimize the critic loss\n",
    "    critic_model_locals[player_idx].train()\n",
    "    critic_optimizers[player_idx].zero_grad()\n",
    "    critic_loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(critic_model_locals[player_idx].parameters(), 1)\n",
    "    critic_optimizers[player_idx].step()\n",
    "\n",
    "    # ---------------------------- update actor ---------------------------- #\n",
    "    # Compute actor loss\n",
    "    actions_pred = actor_model_locals[player_idx](states_tensor)\n",
    "    critic_model_locals[player_idx].eval()\n",
    "    actor_loss = -critic_model_locals[player_idx](states_tensor, actions_pred).mean()\n",
    "    \n",
    "    critic_model_locals[player_idx].train()\n",
    "    critic_model_targets[player_idx].train()\n",
    "    actor_model_targets[player_idx].train()\n",
    "    actor_model_locals[player_idx].train()\n",
    "    \n",
    "    # Minimize the actor loss\n",
    "    actor_optimizers[player_idx].zero_grad()\n",
    "    actor_loss.backward()\n",
    "    actor_optimizers[player_idx].step()\n",
    "\n",
    "    # ----------------------- update target networks ----------------------- #\n",
    "    #use very small Tau and update with every step\n",
    "    soft_update_target(critic_model_locals[player_idx], critic_model_targets[player_idx], tau)\n",
    "    soft_update_target(actor_model_locals[player_idx], actor_model_targets[player_idx], tau)\n",
    "\n",
    "    return critic_loss.item(), actor_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - epsilon: 0.590\n",
      "    total_rewards_0: -0.180, critic_loss_0: 0.00000563, actor_loss_0: 0.021414\n",
      "    total_rewards_1: -0.350, critic_loss_1: 0.00000549, actor_loss_1: 0.020850\n",
      "epoch: 1 - epsilon: 0.580\n",
      "    total_rewards_0: -0.080, critic_loss_0: 0.00000553, actor_loss_0: 0.020890\n",
      "    total_rewards_1: -0.420, critic_loss_1: 0.00000505, actor_loss_1: 0.020627\n",
      "epoch: 2 - epsilon: 0.569\n",
      "    total_rewards_0: -0.070, critic_loss_0: 0.00000547, actor_loss_0: 0.020301\n",
      "    total_rewards_1: -0.350, critic_loss_1: 0.00000647, actor_loss_1: 0.020530\n",
      "epoch: 3 - epsilon: 0.558\n",
      "    total_rewards_0: -0.060, critic_loss_0: 0.00000709, actor_loss_0: 0.019898\n",
      "    total_rewards_1: -0.350, critic_loss_1: 0.00000685, actor_loss_1: 0.020294\n",
      "epoch: 4 - epsilon: 0.547\n",
      "    total_rewards_0: 0.900, critic_loss_0: 0.00001291, actor_loss_0: 0.019092\n",
      "    total_rewards_1: -0.310, critic_loss_1: 0.00001117, actor_loss_1: 0.019589\n",
      "epoch: 5 - epsilon: 0.537\n",
      "    total_rewards_0: 0.610, critic_loss_0: 0.00002227, actor_loss_0: 0.017790\n",
      "    total_rewards_1: 0.440, critic_loss_1: 0.00001561, actor_loss_1: 0.018320\n",
      "epoch: 6 - epsilon: 0.526\n",
      "    total_rewards_0: 0.510, critic_loss_0: 0.00002655, actor_loss_0: 0.016208\n",
      "    total_rewards_1: 1.210, critic_loss_1: 0.00001865, actor_loss_1: 0.016427\n",
      "epoch: 7 - epsilon: 0.516\n",
      "    total_rewards_0: 1.420, critic_loss_0: 0.00003421, actor_loss_0: 0.013987\n",
      "    total_rewards_1: 1.330, critic_loss_1: 0.00002297, actor_loss_1: 0.013152\n",
      "epoch: 8 - epsilon: 0.505\n",
      "    total_rewards_0: 1.410, critic_loss_0: 0.00003373, actor_loss_0: 0.011138\n",
      "    total_rewards_1: 1.600, critic_loss_1: 0.00002626, actor_loss_1: 0.009158\n",
      "epoch: 9 - epsilon: 0.495\n",
      "    total_rewards_0: 1.840, critic_loss_0: 0.00003376, actor_loss_0: 0.007421\n",
      "    total_rewards_1: 1.380, critic_loss_1: 0.00002920, actor_loss_1: 0.004303\n",
      "epoch: 10 - epsilon: 0.485\n",
      "    total_rewards_0: 1.680, critic_loss_0: 0.00003336, actor_loss_0: 0.002750\n",
      "    total_rewards_1: 1.520, critic_loss_1: 0.00003520, actor_loss_1: -0.000741\n",
      "epoch: 11 - epsilon: 0.475\n",
      "    total_rewards_0: 1.480, critic_loss_0: 0.00003516, actor_loss_0: -0.002257\n",
      "    total_rewards_1: 1.610, critic_loss_1: 0.00004076, actor_loss_1: -0.006041\n",
      "epoch: 12 - epsilon: 0.464\n",
      "    total_rewards_0: 1.450, critic_loss_0: 0.00003668, actor_loss_0: -0.007387\n",
      "    total_rewards_1: 1.090, critic_loss_1: 0.00004694, actor_loss_1: -0.011009\n",
      "epoch: 13 - epsilon: 0.454\n",
      "    total_rewards_0: 1.740, critic_loss_0: 0.00004557, actor_loss_0: -0.012424\n",
      "    total_rewards_1: 0.960, critic_loss_1: 0.00005469, actor_loss_1: -0.015558\n",
      "epoch: 14 - epsilon: 0.444\n",
      "    total_rewards_0: 1.610, critic_loss_0: 0.00005862, actor_loss_0: -0.017057\n",
      "    total_rewards_1: 1.280, critic_loss_1: 0.00006234, actor_loss_1: -0.019726\n",
      "epoch: 15 - epsilon: 0.433\n",
      "    total_rewards_0: 1.820, critic_loss_0: 0.00007235, actor_loss_0: -0.021289\n",
      "    total_rewards_1: 1.190, critic_loss_1: 0.00007270, actor_loss_1: -0.023427\n",
      "epoch: 16 - epsilon: 0.423\n",
      "    total_rewards_0: 1.470, critic_loss_0: 0.00008682, actor_loss_0: -0.025431\n",
      "    total_rewards_1: 1.420, critic_loss_1: 0.00008455, actor_loss_1: -0.027486\n",
      "epoch: 17 - epsilon: 0.413\n",
      "    total_rewards_0: 1.550, critic_loss_0: 0.00010553, actor_loss_0: -0.029195\n",
      "    total_rewards_1: 1.550, critic_loss_1: 0.00009779, actor_loss_1: -0.030631\n",
      "epoch: 18 - epsilon: 0.402\n",
      "    total_rewards_0: 1.190, critic_loss_0: 0.00012034, actor_loss_0: -0.032433\n",
      "    total_rewards_1: 1.810, critic_loss_1: 0.00010737, actor_loss_1: -0.033462\n",
      "epoch: 19 - epsilon: 0.392\n",
      "    total_rewards_0: 1.930, critic_loss_0: 0.00013148, actor_loss_0: -0.034802\n",
      "    total_rewards_1: 1.540, critic_loss_1: 0.00011342, actor_loss_1: -0.036090\n",
      "epoch: 20 - epsilon: 0.382\n",
      "    total_rewards_0: 1.140, critic_loss_0: 0.00013812, actor_loss_0: -0.037761\n",
      "    total_rewards_1: 1.500, critic_loss_1: 0.00011359, actor_loss_1: -0.038346\n",
      "epoch: 21 - epsilon: 0.371\n",
      "    total_rewards_0: 1.830, critic_loss_0: 0.00013951, actor_loss_0: -0.040620\n",
      "    total_rewards_1: 1.280, critic_loss_1: 0.00011159, actor_loss_1: -0.040721\n",
      "epoch: 22 - epsilon: 0.361\n",
      "    total_rewards_0: 1.720, critic_loss_0: 0.00013360, actor_loss_0: -0.042790\n",
      "    total_rewards_1: 1.150, critic_loss_1: 0.00010863, actor_loss_1: -0.042702\n",
      "epoch: 23 - epsilon: 0.351\n",
      "    total_rewards_0: 1.740, critic_loss_0: 0.00013017, actor_loss_0: -0.045061\n",
      "    total_rewards_1: 1.500, critic_loss_1: 0.00010011, actor_loss_1: -0.044759\n",
      "epoch: 24 - epsilon: 0.341\n",
      "    total_rewards_0: 1.840, critic_loss_0: 0.00012128, actor_loss_0: -0.047607\n",
      "    total_rewards_1: 1.690, critic_loss_1: 0.00009440, actor_loss_1: -0.046826\n",
      "epoch: 25 - epsilon: 0.330\n",
      "    total_rewards_0: 1.300, critic_loss_0: 0.00011886, actor_loss_0: -0.050157\n",
      "    total_rewards_1: 1.920, critic_loss_1: 0.00008731, actor_loss_1: -0.048820\n",
      "epoch: 26 - epsilon: 0.320\n",
      "    total_rewards_0: 1.690, critic_loss_0: 0.00011101, actor_loss_0: -0.052548\n",
      "    total_rewards_1: 1.290, critic_loss_1: 0.00007904, actor_loss_1: -0.051076\n",
      "epoch: 27 - epsilon: 0.310\n",
      "    total_rewards_0: 1.670, critic_loss_0: 0.00010485, actor_loss_0: -0.054728\n",
      "    total_rewards_1: 1.680, critic_loss_1: 0.00007613, actor_loss_1: -0.053050\n",
      "epoch: 28 - epsilon: 0.299\n",
      "    total_rewards_0: 1.650, critic_loss_0: 0.00009342, actor_loss_0: -0.057005\n",
      "    total_rewards_1: 1.640, critic_loss_1: 0.00006745, actor_loss_1: -0.055091\n",
      "epoch: 29 - epsilon: 0.289\n",
      "    total_rewards_0: 1.660, critic_loss_0: 0.00008270, actor_loss_0: -0.059107\n",
      "    total_rewards_1: 1.910, critic_loss_1: 0.00006071, actor_loss_1: -0.057064\n",
      "epoch: 30 - epsilon: 0.279\n",
      "    total_rewards_0: 1.770, critic_loss_0: 0.00007186, actor_loss_0: -0.061062\n",
      "    total_rewards_1: 1.470, critic_loss_1: 0.00005613, actor_loss_1: -0.058856\n",
      "epoch: 31 - epsilon: 0.269\n",
      "    total_rewards_0: 1.800, critic_loss_0: 0.00006183, actor_loss_0: -0.063214\n",
      "    total_rewards_1: 2.110, critic_loss_1: 0.00005221, actor_loss_1: -0.060768\n",
      "epoch: 32 - epsilon: 0.258\n",
      "    total_rewards_0: 1.820, critic_loss_0: 0.00005015, actor_loss_0: -0.065216\n",
      "    total_rewards_1: 1.630, critic_loss_1: 0.00004957, actor_loss_1: -0.062799\n",
      "epoch: 33 - epsilon: 0.248\n",
      "    total_rewards_0: 1.930, critic_loss_0: 0.00004498, actor_loss_0: -0.067424\n",
      "    total_rewards_1: 1.430, critic_loss_1: 0.00004817, actor_loss_1: -0.064745\n",
      "epoch: 34 - epsilon: 0.238\n",
      "    total_rewards_0: 2.260, critic_loss_0: 0.00004051, actor_loss_0: -0.069596\n",
      "    total_rewards_1: 1.090, critic_loss_1: 0.00004476, actor_loss_1: -0.066658\n",
      "epoch: 35 - epsilon: 0.228\n",
      "    total_rewards_0: 2.040, critic_loss_0: 0.00003757, actor_loss_0: -0.071723\n",
      "    total_rewards_1: 1.640, critic_loss_1: 0.00004518, actor_loss_1: -0.068909\n",
      "epoch: 36 - epsilon: 0.218\n",
      "    total_rewards_0: 1.910, critic_loss_0: 0.00003633, actor_loss_0: -0.073988\n",
      "    total_rewards_1: 1.800, critic_loss_1: 0.00004283, actor_loss_1: -0.071122\n",
      "epoch: 37 - epsilon: 0.207\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00003391, actor_loss_0: -0.075961\n",
      "    total_rewards_1: 1.660, critic_loss_1: 0.00004156, actor_loss_1: -0.073363\n",
      "epoch: 38 - epsilon: 0.197\n",
      "    total_rewards_0: 2.260, critic_loss_0: 0.00003364, actor_loss_0: -0.078142\n",
      "    total_rewards_1: 1.800, critic_loss_1: 0.00004234, actor_loss_1: -0.075837\n",
      "epoch: 39 - epsilon: 0.187\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00003200, actor_loss_0: -0.080285\n",
      "    total_rewards_1: 2.010, critic_loss_1: 0.00003997, actor_loss_1: -0.078094\n",
      "epoch: 40 - epsilon: 0.177\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00003172, actor_loss_0: -0.082646\n",
      "    total_rewards_1: 1.990, critic_loss_1: 0.00003957, actor_loss_1: -0.080600\n",
      "epoch: 41 - epsilon: 0.167\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00003244, actor_loss_0: -0.084524\n",
      "    total_rewards_1: 1.780, critic_loss_1: 0.00003801, actor_loss_1: -0.083110\n",
      "epoch: 42 - epsilon: 0.157\n",
      "    total_rewards_0: 2.160, critic_loss_0: 0.00003162, actor_loss_0: -0.086789\n",
      "    total_rewards_1: 1.790, critic_loss_1: 0.00003734, actor_loss_1: -0.085554\n",
      "epoch: 43 - epsilon: 0.146\n",
      "    total_rewards_0: 2.440, critic_loss_0: 0.00003329, actor_loss_0: -0.088803\n",
      "    total_rewards_1: 1.540, critic_loss_1: 0.00003784, actor_loss_1: -0.087866\n",
      "epoch: 44 - epsilon: 0.136\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00003200, actor_loss_0: -0.090959\n",
      "    total_rewards_1: 1.620, critic_loss_1: 0.00003784, actor_loss_1: -0.090218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45 - epsilon: 0.126\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00003194, actor_loss_0: -0.093125\n",
      "    total_rewards_1: 1.210, critic_loss_1: 0.00003913, actor_loss_1: -0.092724\n",
      "epoch: 46 - epsilon: 0.116\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00003219, actor_loss_0: -0.095190\n",
      "    total_rewards_1: 1.900, critic_loss_1: 0.00003638, actor_loss_1: -0.095350\n",
      "epoch: 47 - epsilon: 0.106\n",
      "    total_rewards_0: 2.500, critic_loss_0: 0.00003211, actor_loss_0: -0.097524\n",
      "    total_rewards_1: 1.330, critic_loss_1: 0.00003925, actor_loss_1: -0.097700\n",
      "epoch: 48 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00003360, actor_loss_0: -0.099479\n",
      "    total_rewards_1: 1.990, critic_loss_1: 0.00003930, actor_loss_1: -0.100025\n",
      "epoch: 49 - epsilon: 0.100\n",
      "    total_rewards_0: 2.260, critic_loss_0: 0.00003444, actor_loss_0: -0.101725\n",
      "    total_rewards_1: 2.110, critic_loss_1: 0.00003851, actor_loss_1: -0.102428\n",
      "epoch: 50 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00003420, actor_loss_0: -0.103710\n",
      "    total_rewards_1: 2.020, critic_loss_1: 0.00003982, actor_loss_1: -0.104467\n",
      "epoch: 51 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00003536, actor_loss_0: -0.105811\n",
      "    total_rewards_1: 1.870, critic_loss_1: 0.00003848, actor_loss_1: -0.106810\n",
      "epoch: 52 - epsilon: 0.100\n",
      "    total_rewards_0: 2.050, critic_loss_0: 0.00003427, actor_loss_0: -0.108018\n",
      "    total_rewards_1: 1.700, critic_loss_1: 0.00003870, actor_loss_1: -0.108830\n",
      "epoch: 53 - epsilon: 0.100\n",
      "    total_rewards_0: 2.360, critic_loss_0: 0.00003388, actor_loss_0: -0.109958\n",
      "    total_rewards_1: 1.910, critic_loss_1: 0.00003909, actor_loss_1: -0.111165\n",
      "epoch: 54 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00003495, actor_loss_0: -0.112093\n",
      "    total_rewards_1: 1.760, critic_loss_1: 0.00003607, actor_loss_1: -0.113235\n",
      "epoch: 55 - epsilon: 0.100\n",
      "    total_rewards_0: 2.160, critic_loss_0: 0.00003520, actor_loss_0: -0.114134\n",
      "    total_rewards_1: 1.870, critic_loss_1: 0.00003668, actor_loss_1: -0.115442\n",
      "epoch: 56 - epsilon: 0.100\n",
      "    total_rewards_0: 2.470, critic_loss_0: 0.00003467, actor_loss_0: -0.115901\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00003423, actor_loss_1: -0.117648\n",
      "epoch: 57 - epsilon: 0.100\n",
      "    total_rewards_0: 2.090, critic_loss_0: 0.00003309, actor_loss_0: -0.117816\n",
      "    total_rewards_1: 1.670, critic_loss_1: 0.00003440, actor_loss_1: -0.119438\n",
      "epoch: 58 - epsilon: 0.100\n",
      "    total_rewards_0: 2.190, critic_loss_0: 0.00003113, actor_loss_0: -0.119591\n",
      "    total_rewards_1: 1.970, critic_loss_1: 0.00003221, actor_loss_1: -0.121465\n",
      "epoch: 59 - epsilon: 0.100\n",
      "    total_rewards_0: 2.690, critic_loss_0: 0.00003169, actor_loss_0: -0.121458\n",
      "    total_rewards_1: 1.550, critic_loss_1: 0.00003140, actor_loss_1: -0.123581\n",
      "epoch: 60 - epsilon: 0.100\n",
      "    total_rewards_0: 2.070, critic_loss_0: 0.00003039, actor_loss_0: -0.123196\n",
      "    total_rewards_1: 1.750, critic_loss_1: 0.00002978, actor_loss_1: -0.125466\n",
      "epoch: 61 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00003027, actor_loss_0: -0.124838\n",
      "    total_rewards_1: 2.000, critic_loss_1: 0.00002951, actor_loss_1: -0.127359\n",
      "epoch: 62 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002946, actor_loss_0: -0.126778\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002836, actor_loss_1: -0.129288\n",
      "epoch: 63 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002824, actor_loss_0: -0.128437\n",
      "    total_rewards_1: 1.540, critic_loss_1: 0.00002689, actor_loss_1: -0.131061\n",
      "epoch: 64 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002749, actor_loss_0: -0.130031\n",
      "    total_rewards_1: 1.440, critic_loss_1: 0.00002624, actor_loss_1: -0.133117\n",
      "epoch: 65 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002732, actor_loss_0: -0.131887\n",
      "    total_rewards_1: 1.780, critic_loss_1: 0.00002719, actor_loss_1: -0.134956\n",
      "epoch: 66 - epsilon: 0.100\n",
      "    total_rewards_0: 2.180, critic_loss_0: 0.00002672, actor_loss_0: -0.133485\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00002675, actor_loss_1: -0.136749\n",
      "epoch: 67 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00002654, actor_loss_0: -0.134905\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002549, actor_loss_1: -0.138641\n",
      "epoch: 68 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00002685, actor_loss_0: -0.136424\n",
      "    total_rewards_1: 1.650, critic_loss_1: 0.00002526, actor_loss_1: -0.140294\n",
      "epoch: 69 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002576, actor_loss_0: -0.137938\n",
      "    total_rewards_1: 1.790, critic_loss_1: 0.00002457, actor_loss_1: -0.142174\n",
      "epoch: 70 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002499, actor_loss_0: -0.139630\n",
      "    total_rewards_1: 2.040, critic_loss_1: 0.00002596, actor_loss_1: -0.143928\n",
      "epoch: 71 - epsilon: 0.100\n",
      "    total_rewards_0: 2.470, critic_loss_0: 0.00002421, actor_loss_0: -0.140904\n",
      "    total_rewards_1: 1.220, critic_loss_1: 0.00002538, actor_loss_1: -0.145746\n",
      "epoch: 72 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002375, actor_loss_0: -0.142574\n",
      "    total_rewards_1: 2.040, critic_loss_1: 0.00002579, actor_loss_1: -0.147575\n",
      "epoch: 73 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002436, actor_loss_0: -0.144021\n",
      "    total_rewards_1: 1.560, critic_loss_1: 0.00002564, actor_loss_1: -0.149305\n",
      "epoch: 74 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00002411, actor_loss_0: -0.145595\n",
      "    total_rewards_1: 1.990, critic_loss_1: 0.00002500, actor_loss_1: -0.151091\n",
      "epoch: 75 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00002420, actor_loss_0: -0.147175\n",
      "    total_rewards_1: 2.220, critic_loss_1: 0.00002586, actor_loss_1: -0.152885\n",
      "epoch: 76 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002350, actor_loss_0: -0.148524\n",
      "    total_rewards_1: 2.030, critic_loss_1: 0.00002678, actor_loss_1: -0.154543\n",
      "epoch: 77 - epsilon: 0.100\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00002368, actor_loss_0: -0.149812\n",
      "    total_rewards_1: 1.910, critic_loss_1: 0.00002739, actor_loss_1: -0.156541\n",
      "epoch: 78 - epsilon: 0.100\n",
      "    total_rewards_0: 2.090, critic_loss_0: 0.00002373, actor_loss_0: -0.151273\n",
      "    total_rewards_1: 2.090, critic_loss_1: 0.00002810, actor_loss_1: -0.158245\n",
      "epoch: 79 - epsilon: 0.100\n",
      "    total_rewards_0: 2.180, critic_loss_0: 0.00002253, actor_loss_0: -0.152896\n",
      "    total_rewards_1: 2.140, critic_loss_1: 0.00002877, actor_loss_1: -0.159984\n",
      "epoch: 80 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00002421, actor_loss_0: -0.154143\n",
      "    total_rewards_1: 2.350, critic_loss_1: 0.00002898, actor_loss_1: -0.161918\n",
      "epoch: 81 - epsilon: 0.100\n",
      "    total_rewards_0: 2.240, critic_loss_0: 0.00002367, actor_loss_0: -0.155484\n",
      "    total_rewards_1: 1.770, critic_loss_1: 0.00002927, actor_loss_1: -0.163542\n",
      "epoch: 82 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00002492, actor_loss_0: -0.156852\n",
      "    total_rewards_1: 1.990, critic_loss_1: 0.00003018, actor_loss_1: -0.165279\n",
      "epoch: 83 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00002555, actor_loss_0: -0.158267\n",
      "    total_rewards_1: 2.100, critic_loss_1: 0.00002921, actor_loss_1: -0.167085\n",
      "epoch: 84 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00002497, actor_loss_0: -0.159633\n",
      "    total_rewards_1: 2.330, critic_loss_1: 0.00002933, actor_loss_1: -0.168925\n",
      "epoch: 85 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002596, actor_loss_0: -0.161041\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002996, actor_loss_1: -0.170645\n",
      "epoch: 86 - epsilon: 0.100\n",
      "    total_rewards_0: 2.350, critic_loss_0: 0.00002387, actor_loss_0: -0.162596\n",
      "    total_rewards_1: 2.250, critic_loss_1: 0.00002904, actor_loss_1: -0.172275\n",
      "epoch: 87 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002540, actor_loss_0: -0.163921\n",
      "    total_rewards_1: 2.090, critic_loss_1: 0.00002859, actor_loss_1: -0.174009\n",
      "epoch: 88 - epsilon: 0.100\n",
      "    total_rewards_0: 2.190, critic_loss_0: 0.00002520, actor_loss_0: -0.164905\n",
      "    total_rewards_1: 2.080, critic_loss_1: 0.00002881, actor_loss_1: -0.175508\n",
      "epoch: 89 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00002435, actor_loss_0: -0.166179\n",
      "    total_rewards_1: 1.650, critic_loss_1: 0.00002833, actor_loss_1: -0.176960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002503, actor_loss_0: -0.167752\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00002611, actor_loss_1: -0.178466\n",
      "epoch: 91 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002386, actor_loss_0: -0.169014\n",
      "    total_rewards_1: 1.870, critic_loss_1: 0.00002683, actor_loss_1: -0.179946\n",
      "epoch: 92 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002316, actor_loss_0: -0.170242\n",
      "    total_rewards_1: 1.780, critic_loss_1: 0.00002641, actor_loss_1: -0.181349\n",
      "epoch: 93 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002413, actor_loss_0: -0.171454\n",
      "    total_rewards_1: 1.560, critic_loss_1: 0.00002499, actor_loss_1: -0.182779\n",
      "epoch: 94 - epsilon: 0.100\n",
      "    total_rewards_0: 2.080, critic_loss_0: 0.00002376, actor_loss_0: -0.172947\n",
      "    total_rewards_1: 1.960, critic_loss_1: 0.00002530, actor_loss_1: -0.184173\n",
      "epoch: 95 - epsilon: 0.100\n",
      "    total_rewards_0: 2.040, critic_loss_0: 0.00002374, actor_loss_0: -0.174067\n",
      "    total_rewards_1: 2.130, critic_loss_1: 0.00002824, actor_loss_1: -0.185944\n",
      "epoch: 96 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002389, actor_loss_0: -0.175252\n",
      "    total_rewards_1: 1.580, critic_loss_1: 0.00002872, actor_loss_1: -0.187547\n",
      "epoch: 97 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002377, actor_loss_0: -0.176396\n",
      "    total_rewards_1: 1.910, critic_loss_1: 0.00002821, actor_loss_1: -0.188954\n",
      "epoch: 98 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002378, actor_loss_0: -0.177720\n",
      "    total_rewards_1: 1.670, critic_loss_1: 0.00002857, actor_loss_1: -0.190438\n",
      "epoch: 99 - epsilon: 0.100\n",
      "    total_rewards_0: 2.090, critic_loss_0: 0.00002338, actor_loss_0: -0.178823\n",
      "    total_rewards_1: 2.210, critic_loss_1: 0.00002655, actor_loss_1: -0.191823\n",
      "epoch: 100 - epsilon: 0.100\n",
      "    total_rewards_0: 2.070, critic_loss_0: 0.00002343, actor_loss_0: -0.179926\n",
      "    total_rewards_1: 2.120, critic_loss_1: 0.00002583, actor_loss_1: -0.193153\n",
      "epoch: 101 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002285, actor_loss_0: -0.180932\n",
      "    total_rewards_1: 2.000, critic_loss_1: 0.00002507, actor_loss_1: -0.194767\n",
      "epoch: 102 - epsilon: 0.100\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00002300, actor_loss_0: -0.182160\n",
      "    total_rewards_1: 2.010, critic_loss_1: 0.00002450, actor_loss_1: -0.195699\n",
      "epoch: 103 - epsilon: 0.100\n",
      "    total_rewards_0: 2.450, critic_loss_0: 0.00002256, actor_loss_0: -0.183235\n",
      "    total_rewards_1: 1.560, critic_loss_1: 0.00002452, actor_loss_1: -0.197211\n",
      "epoch: 104 - epsilon: 0.100\n",
      "    total_rewards_0: 2.240, critic_loss_0: 0.00002236, actor_loss_0: -0.184337\n",
      "    total_rewards_1: 1.910, critic_loss_1: 0.00002500, actor_loss_1: -0.198386\n",
      "epoch: 105 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002182, actor_loss_0: -0.185390\n",
      "    total_rewards_1: 1.550, critic_loss_1: 0.00002330, actor_loss_1: -0.199588\n",
      "epoch: 106 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00002161, actor_loss_0: -0.186480\n",
      "    total_rewards_1: 2.010, critic_loss_1: 0.00002404, actor_loss_1: -0.200943\n",
      "epoch: 107 - epsilon: 0.100\n",
      "    total_rewards_0: 2.470, critic_loss_0: 0.00002046, actor_loss_0: -0.187482\n",
      "    total_rewards_1: 2.020, critic_loss_1: 0.00002300, actor_loss_1: -0.202027\n",
      "epoch: 108 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002139, actor_loss_0: -0.188261\n",
      "    total_rewards_1: 1.820, critic_loss_1: 0.00002243, actor_loss_1: -0.203285\n",
      "epoch: 109 - epsilon: 0.100\n",
      "    total_rewards_0: 2.180, critic_loss_0: 0.00002092, actor_loss_0: -0.189361\n",
      "    total_rewards_1: 2.240, critic_loss_1: 0.00002373, actor_loss_1: -0.204427\n",
      "epoch: 110 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00001997, actor_loss_0: -0.190411\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002265, actor_loss_1: -0.205568\n",
      "epoch: 111 - epsilon: 0.100\n",
      "    total_rewards_0: 2.060, critic_loss_0: 0.00002006, actor_loss_0: -0.191166\n",
      "    total_rewards_1: 2.330, critic_loss_1: 0.00002289, actor_loss_1: -0.206674\n",
      "epoch: 112 - epsilon: 0.100\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00002016, actor_loss_0: -0.192090\n",
      "    total_rewards_1: 2.020, critic_loss_1: 0.00002355, actor_loss_1: -0.207922\n",
      "epoch: 113 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00001973, actor_loss_0: -0.193143\n",
      "    total_rewards_1: 1.860, critic_loss_1: 0.00002328, actor_loss_1: -0.208939\n",
      "epoch: 114 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00001904, actor_loss_0: -0.193988\n",
      "    total_rewards_1: 2.210, critic_loss_1: 0.00002220, actor_loss_1: -0.210082\n",
      "epoch: 115 - epsilon: 0.100\n",
      "    total_rewards_0: 2.300, critic_loss_0: 0.00001942, actor_loss_0: -0.194957\n",
      "    total_rewards_1: 1.670, critic_loss_1: 0.00002325, actor_loss_1: -0.211072\n",
      "epoch: 116 - epsilon: 0.100\n",
      "    total_rewards_0: 2.500, critic_loss_0: 0.00002017, actor_loss_0: -0.195877\n",
      "    total_rewards_1: 1.680, critic_loss_1: 0.00002386, actor_loss_1: -0.212253\n",
      "epoch: 117 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00001904, actor_loss_0: -0.196820\n",
      "    total_rewards_1: 2.120, critic_loss_1: 0.00002317, actor_loss_1: -0.213330\n",
      "epoch: 118 - epsilon: 0.100\n",
      "    total_rewards_0: 2.290, critic_loss_0: 0.00001924, actor_loss_0: -0.197780\n",
      "    total_rewards_1: 1.750, critic_loss_1: 0.00002507, actor_loss_1: -0.214387\n",
      "epoch: 119 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00001906, actor_loss_0: -0.198664\n",
      "    total_rewards_1: 1.810, critic_loss_1: 0.00002411, actor_loss_1: -0.215678\n",
      "epoch: 120 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00001970, actor_loss_0: -0.199486\n",
      "    total_rewards_1: 2.110, critic_loss_1: 0.00002456, actor_loss_1: -0.216657\n",
      "epoch: 121 - epsilon: 0.100\n",
      "    total_rewards_0: 2.300, critic_loss_0: 0.00001840, actor_loss_0: -0.200210\n",
      "    total_rewards_1: 1.790, critic_loss_1: 0.00002411, actor_loss_1: -0.217691\n",
      "epoch: 122 - epsilon: 0.100\n",
      "    total_rewards_0: 2.190, critic_loss_0: 0.00001892, actor_loss_0: -0.201220\n",
      "    total_rewards_1: 2.000, critic_loss_1: 0.00002353, actor_loss_1: -0.218715\n",
      "epoch: 123 - epsilon: 0.100\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00001945, actor_loss_0: -0.201980\n",
      "    total_rewards_1: 2.220, critic_loss_1: 0.00002358, actor_loss_1: -0.219778\n",
      "epoch: 124 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00001843, actor_loss_0: -0.202880\n",
      "    total_rewards_1: 1.780, critic_loss_1: 0.00002390, actor_loss_1: -0.220744\n",
      "epoch: 125 - epsilon: 0.100\n",
      "    total_rewards_0: 2.180, critic_loss_0: 0.00001850, actor_loss_0: -0.203691\n",
      "    total_rewards_1: 1.710, critic_loss_1: 0.00002430, actor_loss_1: -0.221723\n",
      "epoch: 126 - epsilon: 0.100\n",
      "    total_rewards_0: 2.280, critic_loss_0: 0.00001874, actor_loss_0: -0.204630\n",
      "    total_rewards_1: 1.680, critic_loss_1: 0.00002384, actor_loss_1: -0.222802\n",
      "epoch: 127 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00001958, actor_loss_0: -0.205380\n",
      "    total_rewards_1: 1.900, critic_loss_1: 0.00002378, actor_loss_1: -0.223764\n",
      "epoch: 128 - epsilon: 0.100\n",
      "    total_rewards_0: 2.160, critic_loss_0: 0.00001965, actor_loss_0: -0.206213\n",
      "    total_rewards_1: 1.580, critic_loss_1: 0.00002269, actor_loss_1: -0.224567\n",
      "epoch: 129 - epsilon: 0.100\n",
      "    total_rewards_0: 2.790, critic_loss_0: 0.00001804, actor_loss_0: -0.207157\n",
      "    total_rewards_1: 1.990, critic_loss_1: 0.00002320, actor_loss_1: -0.225419\n",
      "epoch: 130 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002074, actor_loss_0: -0.207908\n",
      "    total_rewards_1: 1.670, critic_loss_1: 0.00002335, actor_loss_1: -0.226657\n",
      "epoch: 131 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00001859, actor_loss_0: -0.208603\n",
      "    total_rewards_1: 1.650, critic_loss_1: 0.00002249, actor_loss_1: -0.227421\n",
      "epoch: 132 - epsilon: 0.100\n",
      "    total_rewards_0: 2.470, critic_loss_0: 0.00001887, actor_loss_0: -0.209623\n",
      "    total_rewards_1: 1.690, critic_loss_1: 0.00002401, actor_loss_1: -0.228303\n",
      "epoch: 133 - epsilon: 0.100\n",
      "    total_rewards_0: 2.180, critic_loss_0: 0.00001936, actor_loss_0: -0.210335\n",
      "    total_rewards_1: 2.390, critic_loss_1: 0.00002252, actor_loss_1: -0.229433\n",
      "epoch: 134 - epsilon: 0.100\n",
      "    total_rewards_0: 2.170, critic_loss_0: 0.00001923, actor_loss_0: -0.211138\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002349, actor_loss_1: -0.230043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 135 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00001989, actor_loss_0: -0.211908\n",
      "    total_rewards_1: 1.670, critic_loss_1: 0.00002381, actor_loss_1: -0.231021\n",
      "epoch: 136 - epsilon: 0.100\n",
      "    total_rewards_0: 2.400, critic_loss_0: 0.00001873, actor_loss_0: -0.212427\n",
      "    total_rewards_1: 2.070, critic_loss_1: 0.00002499, actor_loss_1: -0.232044\n",
      "epoch: 137 - epsilon: 0.100\n",
      "    total_rewards_0: 2.070, critic_loss_0: 0.00001962, actor_loss_0: -0.213326\n",
      "    total_rewards_1: 2.080, critic_loss_1: 0.00002275, actor_loss_1: -0.233004\n",
      "epoch: 138 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00001837, actor_loss_0: -0.214041\n",
      "    total_rewards_1: 1.680, critic_loss_1: 0.00002238, actor_loss_1: -0.233817\n",
      "epoch: 139 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00001987, actor_loss_0: -0.214683\n",
      "    total_rewards_1: 1.970, critic_loss_1: 0.00002316, actor_loss_1: -0.234724\n",
      "epoch: 140 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00001947, actor_loss_0: -0.215239\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00002202, actor_loss_1: -0.235555\n",
      "epoch: 141 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00001852, actor_loss_0: -0.215952\n",
      "    total_rewards_1: 2.040, critic_loss_1: 0.00002253, actor_loss_1: -0.236609\n",
      "epoch: 142 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00001935, actor_loss_0: -0.216857\n",
      "    total_rewards_1: 1.880, critic_loss_1: 0.00002177, actor_loss_1: -0.237590\n",
      "epoch: 143 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002074, actor_loss_0: -0.217688\n",
      "    total_rewards_1: 2.320, critic_loss_1: 0.00002245, actor_loss_1: -0.238481\n",
      "epoch: 144 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00001961, actor_loss_0: -0.218068\n",
      "    total_rewards_1: 2.230, critic_loss_1: 0.00002381, actor_loss_1: -0.239211\n",
      "epoch: 145 - epsilon: 0.100\n",
      "    total_rewards_0: 1.930, critic_loss_0: 0.00002013, actor_loss_0: -0.218994\n",
      "    total_rewards_1: 2.020, critic_loss_1: 0.00002255, actor_loss_1: -0.240327\n",
      "epoch: 146 - epsilon: 0.100\n",
      "    total_rewards_0: 2.240, critic_loss_0: 0.00001960, actor_loss_0: -0.219700\n",
      "    total_rewards_1: 2.480, critic_loss_1: 0.00002213, actor_loss_1: -0.241166\n",
      "epoch: 147 - epsilon: 0.100\n",
      "    total_rewards_0: 2.450, critic_loss_0: 0.00001954, actor_loss_0: -0.220402\n",
      "    total_rewards_1: 2.270, critic_loss_1: 0.00002279, actor_loss_1: -0.242148\n",
      "epoch: 148 - epsilon: 0.100\n",
      "    total_rewards_0: 2.260, critic_loss_0: 0.00001976, actor_loss_0: -0.221112\n",
      "    total_rewards_1: 2.140, critic_loss_1: 0.00002279, actor_loss_1: -0.243363\n",
      "epoch: 149 - epsilon: 0.100\n",
      "    total_rewards_0: 2.690, critic_loss_0: 0.00002022, actor_loss_0: -0.221861\n",
      "    total_rewards_1: 2.150, critic_loss_1: 0.00002093, actor_loss_1: -0.243970\n",
      "epoch: 150 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002038, actor_loss_0: -0.222794\n",
      "    total_rewards_1: 2.130, critic_loss_1: 0.00002281, actor_loss_1: -0.245058\n",
      "epoch: 151 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00002029, actor_loss_0: -0.223406\n",
      "    total_rewards_1: 2.360, critic_loss_1: 0.00002282, actor_loss_1: -0.246018\n",
      "epoch: 152 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00002035, actor_loss_0: -0.224264\n",
      "    total_rewards_1: 2.030, critic_loss_1: 0.00002205, actor_loss_1: -0.246998\n",
      "epoch: 153 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00001930, actor_loss_0: -0.224967\n",
      "    total_rewards_1: 2.450, critic_loss_1: 0.00002274, actor_loss_1: -0.248039\n",
      "epoch: 154 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002090, actor_loss_0: -0.225615\n",
      "    total_rewards_1: 2.250, critic_loss_1: 0.00002229, actor_loss_1: -0.249028\n",
      "epoch: 155 - epsilon: 0.100\n",
      "    total_rewards_0: 2.500, critic_loss_0: 0.00002158, actor_loss_0: -0.226320\n",
      "    total_rewards_1: 2.230, critic_loss_1: 0.00002317, actor_loss_1: -0.250224\n",
      "epoch: 156 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002078, actor_loss_0: -0.227111\n",
      "    total_rewards_1: 2.350, critic_loss_1: 0.00002153, actor_loss_1: -0.251452\n",
      "epoch: 157 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002080, actor_loss_0: -0.227675\n",
      "    total_rewards_1: 2.270, critic_loss_1: 0.00002322, actor_loss_1: -0.252371\n",
      "epoch: 158 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002086, actor_loss_0: -0.228356\n",
      "    total_rewards_1: 2.430, critic_loss_1: 0.00002260, actor_loss_1: -0.253425\n",
      "epoch: 159 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002080, actor_loss_0: -0.229081\n",
      "    total_rewards_1: 2.140, critic_loss_1: 0.00002528, actor_loss_1: -0.254631\n",
      "epoch: 160 - epsilon: 0.100\n",
      "    total_rewards_0: 2.700, critic_loss_0: 0.00002230, actor_loss_0: -0.229827\n",
      "    total_rewards_1: 2.350, critic_loss_1: 0.00002186, actor_loss_1: -0.255701\n",
      "epoch: 161 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002155, actor_loss_0: -0.230606\n",
      "    total_rewards_1: 2.560, critic_loss_1: 0.00002431, actor_loss_1: -0.256701\n",
      "epoch: 162 - epsilon: 0.100\n",
      "    total_rewards_0: 2.080, critic_loss_0: 0.00002235, actor_loss_0: -0.231288\n",
      "    total_rewards_1: 2.430, critic_loss_1: 0.00002370, actor_loss_1: -0.257737\n",
      "epoch: 163 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002293, actor_loss_0: -0.231815\n",
      "    total_rewards_1: 2.240, critic_loss_1: 0.00002294, actor_loss_1: -0.258707\n",
      "epoch: 164 - epsilon: 0.100\n",
      "    total_rewards_0: 2.300, critic_loss_0: 0.00002377, actor_loss_0: -0.232446\n",
      "    total_rewards_1: 2.210, critic_loss_1: 0.00002346, actor_loss_1: -0.259574\n",
      "epoch: 165 - epsilon: 0.100\n",
      "    total_rewards_0: 2.360, critic_loss_0: 0.00002311, actor_loss_0: -0.233253\n",
      "    total_rewards_1: 2.120, critic_loss_1: 0.00002573, actor_loss_1: -0.260508\n",
      "epoch: 166 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002396, actor_loss_0: -0.233771\n",
      "    total_rewards_1: 1.810, critic_loss_1: 0.00002482, actor_loss_1: -0.261424\n",
      "epoch: 167 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002244, actor_loss_0: -0.234426\n",
      "    total_rewards_1: 2.240, critic_loss_1: 0.00002575, actor_loss_1: -0.262596\n",
      "epoch: 168 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002447, actor_loss_0: -0.234941\n",
      "    total_rewards_1: 2.130, critic_loss_1: 0.00002630, actor_loss_1: -0.263531\n",
      "epoch: 169 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002328, actor_loss_0: -0.235628\n",
      "    total_rewards_1: 2.430, critic_loss_1: 0.00002759, actor_loss_1: -0.264434\n",
      "epoch: 170 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002318, actor_loss_0: -0.236257\n",
      "    total_rewards_1: 2.090, critic_loss_1: 0.00002723, actor_loss_1: -0.265529\n",
      "epoch: 171 - epsilon: 0.100\n",
      "    total_rewards_0: 2.370, critic_loss_0: 0.00002399, actor_loss_0: -0.236890\n",
      "    total_rewards_1: 2.110, critic_loss_1: 0.00002633, actor_loss_1: -0.266176\n",
      "epoch: 172 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002416, actor_loss_0: -0.237514\n",
      "    total_rewards_1: 2.000, critic_loss_1: 0.00002726, actor_loss_1: -0.267241\n",
      "epoch: 173 - epsilon: 0.100\n",
      "    total_rewards_0: 2.680, critic_loss_0: 0.00002446, actor_loss_0: -0.237968\n",
      "    total_rewards_1: 1.780, critic_loss_1: 0.00002898, actor_loss_1: -0.268315\n",
      "epoch: 174 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002407, actor_loss_0: -0.238612\n",
      "    total_rewards_1: 2.360, critic_loss_1: 0.00002893, actor_loss_1: -0.269112\n",
      "epoch: 175 - epsilon: 0.100\n",
      "    total_rewards_0: 2.390, critic_loss_0: 0.00002390, actor_loss_0: -0.239276\n",
      "    total_rewards_1: 1.980, critic_loss_1: 0.00002824, actor_loss_1: -0.270212\n",
      "epoch: 176 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002464, actor_loss_0: -0.239784\n",
      "    total_rewards_1: 2.150, critic_loss_1: 0.00002796, actor_loss_1: -0.270804\n",
      "epoch: 177 - epsilon: 0.100\n",
      "    total_rewards_0: 2.270, critic_loss_0: 0.00002443, actor_loss_0: -0.240147\n",
      "    total_rewards_1: 2.110, critic_loss_1: 0.00002733, actor_loss_1: -0.271767\n",
      "epoch: 178 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002329, actor_loss_0: -0.240876\n",
      "    total_rewards_1: 2.040, critic_loss_1: 0.00003089, actor_loss_1: -0.272670\n",
      "epoch: 179 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002222, actor_loss_0: -0.241345\n",
      "    total_rewards_1: 2.260, critic_loss_1: 0.00002659, actor_loss_1: -0.273299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 180 - epsilon: 0.100\n",
      "    total_rewards_0: 2.690, critic_loss_0: 0.00002101, actor_loss_0: -0.241987\n",
      "    total_rewards_1: 2.130, critic_loss_1: 0.00002962, actor_loss_1: -0.273841\n",
      "epoch: 181 - epsilon: 0.100\n",
      "    total_rewards_0: 2.070, critic_loss_0: 0.00002210, actor_loss_0: -0.242509\n",
      "    total_rewards_1: 2.230, critic_loss_1: 0.00002718, actor_loss_1: -0.274712\n",
      "epoch: 182 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00002289, actor_loss_0: -0.242871\n",
      "    total_rewards_1: 2.120, critic_loss_1: 0.00002875, actor_loss_1: -0.275501\n",
      "epoch: 183 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002326, actor_loss_0: -0.243444\n",
      "    total_rewards_1: 2.010, critic_loss_1: 0.00002676, actor_loss_1: -0.276106\n",
      "epoch: 184 - epsilon: 0.100\n",
      "    total_rewards_0: 2.380, critic_loss_0: 0.00002219, actor_loss_0: -0.243928\n",
      "    total_rewards_1: 2.090, critic_loss_1: 0.00002740, actor_loss_1: -0.276940\n",
      "epoch: 185 - epsilon: 0.100\n",
      "    total_rewards_0: 2.250, critic_loss_0: 0.00002250, actor_loss_0: -0.244524\n",
      "    total_rewards_1: 2.130, critic_loss_1: 0.00002712, actor_loss_1: -0.277498\n",
      "epoch: 186 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002257, actor_loss_0: -0.244687\n",
      "    total_rewards_1: 2.200, critic_loss_1: 0.00002623, actor_loss_1: -0.278146\n",
      "epoch: 187 - epsilon: 0.100\n",
      "    total_rewards_0: 2.570, critic_loss_0: 0.00002249, actor_loss_0: -0.245442\n",
      "    total_rewards_1: 2.000, critic_loss_1: 0.00002654, actor_loss_1: -0.278782\n",
      "epoch: 188 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00002165, actor_loss_0: -0.245904\n",
      "    total_rewards_1: 2.330, critic_loss_1: 0.00002417, actor_loss_1: -0.279379\n",
      "epoch: 189 - epsilon: 0.100\n",
      "    total_rewards_0: 2.690, critic_loss_0: 0.00002130, actor_loss_0: -0.246446\n",
      "    total_rewards_1: 2.190, critic_loss_1: 0.00002426, actor_loss_1: -0.280197\n",
      "epoch: 190 - epsilon: 0.100\n",
      "    total_rewards_0: 2.580, critic_loss_0: 0.00002114, actor_loss_0: -0.246918\n",
      "    total_rewards_1: 2.080, critic_loss_1: 0.00002316, actor_loss_1: -0.281173\n",
      "epoch: 191 - epsilon: 0.100\n",
      "    total_rewards_0: 2.600, critic_loss_0: 0.00002264, actor_loss_0: -0.247450\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00002385, actor_loss_1: -0.281656\n",
      "epoch: 192 - epsilon: 0.100\n",
      "    total_rewards_0: 2.700, critic_loss_0: 0.00002221, actor_loss_0: -0.247960\n",
      "    total_rewards_1: 2.340, critic_loss_1: 0.00002311, actor_loss_1: -0.282364\n",
      "epoch: 193 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002364, actor_loss_0: -0.248515\n",
      "    total_rewards_1: 2.100, critic_loss_1: 0.00002048, actor_loss_1: -0.283149\n",
      "epoch: 194 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00002268, actor_loss_0: -0.249169\n",
      "    total_rewards_1: 2.260, critic_loss_1: 0.00002318, actor_loss_1: -0.283957\n",
      "epoch: 195 - epsilon: 0.100\n",
      "    total_rewards_0: 2.570, critic_loss_0: 0.00002068, actor_loss_0: -0.249535\n",
      "    total_rewards_1: 2.040, critic_loss_1: 0.00002162, actor_loss_1: -0.284704\n",
      "epoch: 196 - epsilon: 0.100\n",
      "    total_rewards_0: 2.490, critic_loss_0: 0.00002258, actor_loss_0: -0.250189\n",
      "    total_rewards_1: 2.100, critic_loss_1: 0.00002167, actor_loss_1: -0.285464\n",
      "epoch: 197 - epsilon: 0.100\n",
      "    total_rewards_0: 2.690, critic_loss_0: 0.00002227, actor_loss_0: -0.250415\n",
      "    total_rewards_1: 1.890, critic_loss_1: 0.00002109, actor_loss_1: -0.286209\n",
      "epoch: 198 - epsilon: 0.100\n",
      "    total_rewards_0: 2.480, critic_loss_0: 0.00002241, actor_loss_0: -0.251198\n",
      "    total_rewards_1: 2.230, critic_loss_1: 0.00002092, actor_loss_1: -0.287030\n",
      "epoch: 199 - epsilon: 0.100\n",
      "    total_rewards_0: 2.590, critic_loss_0: 0.00001970, actor_loss_0: -0.251698\n",
      "    total_rewards_1: 2.240, critic_loss_1: 0.00002272, actor_loss_1: -0.287704\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "epochs = 200 # 5\n",
    "steps_per_epoch = 1000\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "batch_size = 128\n",
    "epsilon_decay_steps = buffer_length\n",
    "epsilon_max = 0.75\n",
    "epsilon_min = 0.1\n",
    "current_state = reset_game(env, brain_name)\n",
    "\n",
    "with active_session():\n",
    "    for epoch in range(epochs):\n",
    "        total_actor_loss_0 = 0.0\n",
    "        total_critic_loss_0 = 0.0    \n",
    "        total_actor_loss_1 = 0.0\n",
    "        total_critic_loss_1 = 0.0    \n",
    "        total_rewards_0 = 0\n",
    "        total_rewards_1 = 0\n",
    "        epsilon = max(epsilon_min, epsilon_max - len(replay_buffer)/epsilon_decay_steps)\n",
    "\n",
    "        for game_step in range(steps_per_epoch):\n",
    "            #play a game\n",
    "            #total_rewards = play_game(env, brain_name, replay_buffer, actor_model_local)\n",
    "            current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "            total_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "            total_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "\n",
    "            #if the game is done, reset and continue\n",
    "            if np.any(replay_buffer[-1].dones):\n",
    "                # new game\n",
    "                current_state = reset_game(env, brain_name)\n",
    "                current_state = env_step(env, brain_name, current_state, replay_buffer, actor_model_locals[0], actor_model_locals[1], epsilon)\n",
    "                total_rewards_0 += np.sum(replay_buffer[-1].rewards[0])\n",
    "                total_rewards_1 += np.sum(replay_buffer[-1].rewards[1])\n",
    "\n",
    "            if len(replay_buffer) < 10000:\n",
    "                continue  \n",
    "\n",
    "            critic_loss_0, actor_loss_0  = train_player(0, batch_size, gamma, tau)\n",
    "            critic_loss_1, actor_loss_1  = train_player(1, batch_size, gamma, tau)\n",
    "\n",
    "            total_actor_loss_0 += actor_loss_0\n",
    "            total_actor_loss_1 += actor_loss_1\n",
    "            total_critic_loss_0 += critic_loss_0\n",
    "            total_critic_loss_1 += critic_loss_1\n",
    "\n",
    "        print(\"epoch: {} - epsilon: {:.3f}\".format(epoch, epsilon))\n",
    "        print(\"    total_rewards_0: {:.3f}, critic_loss_0: {:.8f}, actor_loss_0: {:.6f}\".format(total_rewards_0, total_critic_loss_0/steps_per_epoch, total_actor_loss_0/steps_per_epoch))\n",
    "        print(\"    total_rewards_1: {:.3f}, critic_loss_1: {:.8f}, actor_loss_1: {:.6f}\".format(total_rewards_1, total_critic_loss_1/steps_per_epoch, total_actor_loss_1/steps_per_epoch))\n",
    "\n",
    "    torch.save(actor_model_locals[0].state_dict(),  \"actor_model_local_0.pt\")\n",
    "    torch.save(actor_model_targets[0].state_dict(), \"actor_model_target_0.pt\")\n",
    "    torch.save(critic_model_locals[0].state_dict(), \"critic_model_local_0.pt\")\n",
    "    torch.save(critic_model_targets[0].state_dict(),\"critic_model_target_0.pt\")\n",
    "\n",
    "    torch.save(actor_model_locals[1].state_dict(),  \"actor_model_local_1.pt\")\n",
    "    torch.save(actor_model_targets[1].state_dict(), \"actor_model_target_1.pt\")\n",
    "    torch.save(critic_model_locals[1].state_dict(), \"critic_model_local_1.pt\")\n",
    "    torch.save(critic_model_targets[1].state_dict(),\"critic_model_target_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game 1, total_rewards_agent_0: 0.400, total_rewards_agent_1: 0.290, game steps: 180\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0\n",
    "total_game_count = 1\n",
    "total_rewards_over_all_games_agent_0 = 0\n",
    "total_rewards_over_all_games_agent_1 = 0\n",
    "total_max_over_all_games = 0\n",
    "\n",
    "for game_count in range(total_game_count):\n",
    "    total_rewards_agent_0 = 0.0\n",
    "    total_rewards_agent_1 = 0.0\n",
    "\n",
    "    current_state = reset_game(env, brain_name)\n",
    "    replay = []\n",
    "    \n",
    "    for i in range(2000):\n",
    "        current_state = env_step(env, brain_name, current_state, replay, actor_model_locals[0], actor_model_locals[1], epsilon, logging=False)\n",
    "        total_rewards_agent_0 += replay[-1].rewards[0]\n",
    "        total_rewards_agent_1 += replay[-1].rewards[1]\n",
    "\n",
    "        total_rewards_over_all_games_agent_0 += total_rewards_agent_0\n",
    "        total_rewards_over_all_games_agent_1 += total_rewards_agent_1\n",
    "        \n",
    "        if np.any(replay[-1].dones):\n",
    "            break\n",
    "        \n",
    "    print(\"game {}, total_rewards_agent_0: {:.3f}, total_rewards_agent_1: {:.3f}, game steps: {}\".format(game_count+1, total_rewards_agent_0, total_rewards_agent_1, len(replay)))\n",
    "    \n",
    "#print(\"Average reward over {} games: {:.3f}\".format(total_game_count, total_rewards_over_all_games/total_game_count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    stepInfo = replay_buffer[i]\n",
    "    print(\"\\nstep_number: {}\".format(stepInfo.step_number))\n",
    "    print(\"states: {}\".format(stepInfo.states))\n",
    "    print(\"actions: {}\".format(stepInfo.actions))\n",
    "    print(\"rewards: {}\".format(stepInfo.rewards))\n",
    "    print(\"next_states: {}\".format(stepInfo.next_states))\n",
    "    print(\"dones: {}\".format(stepInfo.dones))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
